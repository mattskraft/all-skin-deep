{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:43:20.011306: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 21:43:20.023268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745523800.036264  572260 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745523800.039890  572260 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745523800.049632  572260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745523800.049649  572260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745523800.049650  572260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745523800.049651  572260 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-24 21:43:20.052698: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import (TEST_DIR, MODEL_DIR)\n",
    "from model_utils import (create_regular_generator,\n",
    "                         load_model_from_path,\n",
    "                         setup_tf_device)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(df):\n",
    "    \"\"\"Identify loss and metric columns in the DataFrame\"\"\"\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Try to find train and val loss columns\n",
    "    train_loss = [col for col in columns if 'loss' in col.lower() and 'val' not in col.lower()]\n",
    "    val_loss = [col for col in columns if 'val' in col.lower() and 'loss' in col.lower()]\n",
    "    \n",
    "    train_loss = train_loss[0] if train_loss else None\n",
    "    val_loss = val_loss[0] if val_loss else None\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "\n",
    "def find_metric_columns(df, metric_name):\n",
    "    \"\"\"Find training and validation columns for the specified metric\"\"\"\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Find the training metric column\n",
    "    train_metric_options = [\n",
    "        metric_name,\n",
    "        f\"train_{metric_name}\",\n",
    "        f\"{metric_name}\"\n",
    "    ]\n",
    "    \n",
    "    # Find the validation metric column\n",
    "    val_metric_options = [\n",
    "        f\"val_{metric_name}\",\n",
    "        f\"validation_{metric_name}\"\n",
    "    ]\n",
    "    \n",
    "    train_metric = None\n",
    "    for option in train_metric_options:\n",
    "        if option in columns:\n",
    "            train_metric = option\n",
    "            break\n",
    "    \n",
    "    val_metric = None\n",
    "    for option in val_metric_options:\n",
    "        if option in columns:\n",
    "            val_metric = option\n",
    "            break\n",
    "    \n",
    "    return train_metric, val_metric\n",
    "\n",
    "def plot_training_histories(history_files, labels=None, metric_name='f1_macro', \n",
    "                           output_dir='plots', figsize=(18, 14), save_plot=False):\n",
    "    \"\"\"\n",
    "    Plot training histories from CSV files with first history separate from others.\n",
    "    Ensures equal y-limits for loss plots and metric plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history_files : list\n",
    "        List of paths to history CSV files (at least 2 files expected)\n",
    "    labels : list, optional\n",
    "        Labels for each history file. If None, filenames are used.\n",
    "    metric_name : str, optional\n",
    "        Metric to plot along with loss (default: 'f1_macro')\n",
    "    output_dir : str, optional\n",
    "        Directory to save plots (default: 'plots')\n",
    "    figsize : tuple, optional\n",
    "        Figure size (width, height) (default: (18, 14))\n",
    "    save_plot : bool, optional\n",
    "        Whether to save the plot to a file (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The created figure\n",
    "    \"\"\"\n",
    "    # Load history files\n",
    "    histories = []\n",
    "    for file in history_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Add epoch column if not present\n",
    "            if 'epoch' not in df.columns:\n",
    "                df['epoch'] = np.arange(1, len(df) + 1)\n",
    "            histories.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            histories.append(None)\n",
    "    \n",
    "    # Get labels\n",
    "    if labels is None:\n",
    "        labels = [Path(file).stem for file in history_files]\n",
    "    \n",
    "    # Ensure we have the same number of labels as histories\n",
    "    if len(labels) < len(history_files):\n",
    "        # Add default labels if needed\n",
    "        additional_labels = [f\"Model {i+1}\" for i in range(len(labels), len(history_files))]\n",
    "        labels.extend(additional_labels)\n",
    "    \n",
    "    # Create figure and subplots - 2 rows, 2 columns\n",
    "    fig, axs = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Create references to the four subplots\n",
    "    ax_loss_first = axs[0, 0]    # Top-left for first model's loss\n",
    "    ax_metric_first = axs[1, 0]  # Bottom-left for first model's metric\n",
    "    ax_loss_others = axs[0, 1]   # Top-right for other models' loss\n",
    "    ax_metric_others = axs[1, 1] # Bottom-right for other models' metric\n",
    "    \n",
    "    # Set titles and labels\n",
    "    ax_loss_first.set_title(f'{labels[0]} - Training and Validation Loss', fontsize=16)\n",
    "    ax_loss_first.set_ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    ax_metric_first.set_title(f'{labels[0]} - Training and Validation {metric_name.upper()}', fontsize=16)\n",
    "    ax_metric_first.set_xlabel('Epoch', fontsize=14)\n",
    "    ax_metric_first.set_ylabel(metric_name.upper(), fontsize=14)\n",
    "    \n",
    "    ax_loss_others.set_title(f'Models Comparison - Training and Validation Loss', fontsize=16)\n",
    "    ax_loss_others.set_ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    ax_metric_others.set_title(f'Models Comparison - Training and Validation {metric_name.upper()}', fontsize=16)\n",
    "    ax_metric_others.set_xlabel('Epoch', fontsize=14)\n",
    "    ax_metric_others.set_ylabel(metric_name.upper(), fontsize=14)\n",
    "    \n",
    "    # Colors for different histories\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    # First, collect all data to determine global min/max values for consistent y-axis limits\n",
    "    loss_min, loss_max = float('inf'), float('-inf')\n",
    "    metric_min, metric_max = float('inf'), float('-inf')\n",
    "    \n",
    "    for i, history_df in enumerate(histories):\n",
    "        if history_df is None:\n",
    "            continue\n",
    "            \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(history_df)\n",
    "        train_metric, val_metric = find_metric_columns(history_df, metric_name)\n",
    "        \n",
    "        # Update loss min/max\n",
    "        if train_loss:\n",
    "            loss_min = min(loss_min, history_df[train_loss].min())\n",
    "            loss_max = max(loss_max, history_df[train_loss].max())\n",
    "        \n",
    "        if val_loss:\n",
    "            loss_min = min(loss_min, history_df[val_loss].min())\n",
    "            loss_max = max(loss_max, history_df[val_loss].max())\n",
    "            \n",
    "        # Update metric min/max\n",
    "        if train_metric:\n",
    "            metric_min = min(metric_min, history_df[train_metric].min())\n",
    "            metric_max = max(metric_max, history_df[train_metric].max())\n",
    "        \n",
    "        if val_metric:\n",
    "            metric_min = min(metric_min, history_df[val_metric].min())\n",
    "            metric_max = max(metric_max, history_df[val_metric].max())\n",
    "    \n",
    "    # Add some padding to the ranges (5% padding)\n",
    "    loss_range = loss_max - loss_min\n",
    "    loss_min -= loss_range * 0.05\n",
    "    loss_max += loss_range * 0.05\n",
    "    \n",
    "    metric_range = metric_max - metric_min\n",
    "    metric_min -= metric_range * 0.05\n",
    "    metric_max += metric_range * 0.05\n",
    "    \n",
    "    # Plot first history\n",
    "    if histories[0] is not None:\n",
    "        first_df = histories[0]\n",
    "        color = colors[0]\n",
    "        \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(first_df)\n",
    "        train_metric, val_metric = find_metric_columns(first_df, metric_name)\n",
    "        \n",
    "        # Plot training and validation loss for first model\n",
    "        if train_loss:\n",
    "            ax_loss_first.plot(first_df['epoch'], first_df[train_loss], \n",
    "                            color=color, linestyle='-', marker='o', markersize=4, \n",
    "                            label='Train')\n",
    "        \n",
    "        if val_loss:\n",
    "            ax_loss_first.plot(first_df['epoch'], first_df[val_loss], \n",
    "                            color=color, linestyle='--', marker='s', markersize=4, \n",
    "                            label='Validation')\n",
    "        \n",
    "        # Plot training and validation metric for first model\n",
    "        if train_metric:\n",
    "            ax_metric_first.plot(first_df['epoch'], first_df[train_metric], \n",
    "                                color=color, linestyle='-', marker='o', markersize=4, \n",
    "                                label='Train')\n",
    "        \n",
    "        if val_metric:\n",
    "            ax_metric_first.plot(first_df['epoch'], first_df[val_metric], \n",
    "                                color=color, linestyle='--', marker='s', markersize=4, \n",
    "                                label='Validation')\n",
    "    \n",
    "    # Plot second and third histories (and any additional ones) together\n",
    "    for i in range(1, len(histories)):\n",
    "        history_df = histories[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        if history_df is None:\n",
    "            continue\n",
    "        \n",
    "        # Get color for this history\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(history_df)\n",
    "        train_metric, val_metric = find_metric_columns(history_df, metric_name)\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        if train_loss:\n",
    "            ax_loss_others.plot(history_df['epoch'], history_df[train_loss], \n",
    "                              color=color, linestyle='-', marker='o', markersize=4, \n",
    "                              label=f'{label} - Train')\n",
    "        \n",
    "        if val_loss:\n",
    "            ax_loss_others.plot(history_df['epoch'], history_df[val_loss], \n",
    "                              color=color, linestyle='--', marker='s', markersize=4, \n",
    "                              label=f'{label} - Val')\n",
    "        \n",
    "        # Plot training and validation metric\n",
    "        if train_metric:\n",
    "            ax_metric_others.plot(history_df['epoch'], history_df[train_metric], \n",
    "                                color=color, linestyle='-', marker='o', markersize=4, \n",
    "                                label=f'{label} - Train')\n",
    "        \n",
    "        if val_metric:\n",
    "            ax_metric_others.plot(history_df['epoch'], history_df[val_metric], \n",
    "                                color=color, linestyle='--', marker='s', markersize=4, \n",
    "                                label=f'{label} - Val')\n",
    "    \n",
    "    # Set consistent y-limits for all loss plots and all metric plots\n",
    "    ax_loss_first.set_ylim(loss_min, loss_max)\n",
    "    ax_loss_others.set_ylim(loss_min, loss_max)\n",
    "    \n",
    "    ax_metric_first.set_ylim(metric_min, metric_max)\n",
    "    ax_metric_others.set_ylim(metric_min, metric_max)\n",
    "    \n",
    "    # Add legends\n",
    "    ax_loss_first.legend(loc='best', fontsize=12)\n",
    "    ax_metric_first.legend(loc='best', fontsize=12)\n",
    "    ax_loss_others.legend(loc='best', fontsize=12)\n",
    "    ax_metric_others.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    # Set grid\n",
    "    ax_loss_first.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_metric_first.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_loss_others.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_metric_others.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save_plot:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'training_history_split_{metric_name}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_files = [\n",
    "    \"/home/matthias/projects/disc/models/skin_lesion_model_20250411_121210-20250413T151506Z-001/skin_lesion_model_20250411_121210/training_history.csv\", \n",
    "    \"/home/matthias/projects/disc/models/cross_style_results-20250413T151536Z-001/cross_style_results/history_round1.csv\", \n",
    "    \"/home/matthias/projects/disc/models/cross_style_results-20250413T151536Z-001/cross_style_results/history_round2.csv\", \n",
    "]\n",
    "labels = [\"Fine-tuning on Natural Images\", \"Cross Fine-tuned (Round 1)\", \"Cross Fine-tuned (Round 2)\"]\n",
    "fig = plot_training_histories(history_files, labels, metric_name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Using CPU.\n",
      "Found 1121 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /home/matthias/projects/all-skin-deep/models/finetune_orig_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 21:43:30.152594: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m model_paths:\n\u001b[0;32m---> 11\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     y_prob_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_generator)\n\u001b[1;32m     13\u001b[0m     y_prob\u001b[38;5;241m.\u001b[39mappend(y_prob_)\n",
      "File \u001b[0;32m~/projects/all-skin-deep/notebooks/../model_utils.py:233\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m    231\u001b[0m custom_objects \u001b[38;5;241m=\u001b[39m CUSTOM_OBJECTS\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    232\u001b[0m custom_objects[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfocal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_loss_function(class_weights\u001b[38;5;241m=\u001b[39mCLASS_WEIGHTS)\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/disc/lib/python3.10/site-packages/keras/src/saving/saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    190\u001b[0m         filepath,\n\u001b[1;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    194\u001b[0m     )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/disc/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/disc/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:148\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    146\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _deserialize_nested_config(losses\u001b[38;5;241m.\u001b[39mdeserialize, loss_config)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_compile_arguments_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Recover metrics.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/disc/lib/python3.10/site-packages/keras/src/legacy/saving/saving_utils.py:248\u001b[0m, in \u001b[0;36m_resolve_compile_arguments_compat\u001b[0;34m(obj, obj_config, module)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resolves backwards compatibility issues with training config arguments.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03mThis helper function accepts built-in Keras modules such as optimizers,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mthis does nothing.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m obj \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mALL_OBJECTS_DICT:\n\u001b[0;32m--> 248\u001b[0m     obj \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mget(\u001b[43mobj_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "device = setup_tf_device()\n",
    "test_generator = create_regular_generator(TEST_DIR, with_augment=False, shuffle=False)\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "y_true = test_generator.classes\n",
    "\n",
    "model_paths = [MODEL_DIR / \"finetune_orig_best.h5\",\n",
    "               MODEL_DIR / \"finetune_cross_round_2.h5\"]\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "for path in model_paths:\n",
    "    model = load_model_from_path(path)\n",
    "    y_prob_ = model.predict(test_generator)\n",
    "    y_prob.append(y_prob_)\n",
    "    y_pred.append(np.argmax(y_prob_, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
