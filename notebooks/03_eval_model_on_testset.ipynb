{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 03 – Evaluate Models\n",
    "\n",
    "In this notebook we’ll:\n",
    "- have a look at the training history of both stages\n",
    "- run inference on both models using the test set\n",
    "- for each model, plot some metrics such as\n",
    "    - confusion matrices\n",
    "    - class-wise and macro F1 scores\n",
    "    - class-wise and top-3 classification accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.getcwd())\n",
    "sys.path.append('../scripts/')\n",
    "sys.path.append('../scripts/training/')\n",
    "from config import (TEST_DIR, MODEL_DIR, PLOT_DIR)\n",
    "from training_utils import (create_regular_generator,\n",
    "                         load_model_from_path)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_columns(df):\n",
    "    \"\"\"Identify loss and metric columns in the DataFrame\"\"\"\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Try to find train and val loss columns\n",
    "    train_loss = [col for col in columns if 'loss' in col.lower() and 'val' not in col.lower()]\n",
    "    val_loss = [col for col in columns if 'val' in col.lower() and 'loss' in col.lower()]\n",
    "    \n",
    "    train_loss = train_loss[0] if train_loss else None\n",
    "    val_loss = val_loss[0] if val_loss else None\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "\n",
    "def find_metric_columns(df, metric_name):\n",
    "    \"\"\"Find training and validation columns for the specified metric\"\"\"\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Find the training metric column\n",
    "    train_metric_options = [\n",
    "        metric_name,\n",
    "        f\"train_{metric_name}\",\n",
    "        f\"{metric_name}\"\n",
    "    ]\n",
    "    \n",
    "    # Find the validation metric column\n",
    "    val_metric_options = [\n",
    "        f\"val_{metric_name}\",\n",
    "        f\"validation_{metric_name}\"\n",
    "    ]\n",
    "    \n",
    "    train_metric = None\n",
    "    for option in train_metric_options:\n",
    "        if option in columns:\n",
    "            train_metric = option\n",
    "            break\n",
    "    \n",
    "    val_metric = None\n",
    "    for option in val_metric_options:\n",
    "        if option in columns:\n",
    "            val_metric = option\n",
    "            break\n",
    "    \n",
    "    return train_metric, val_metric\n",
    "\n",
    "def plot_training_histories(history_files, labels=None, metric_name='f1_macro', \n",
    "                           output_dir='plots', figsize=(18, 14), save_plot=False):\n",
    "    \"\"\"\n",
    "    Plot training histories from CSV files with first history separate from others.\n",
    "    Ensures equal y-limits for loss plots and metric plots.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history_files : list\n",
    "        List of paths to history CSV files (at least 2 files expected)\n",
    "    labels : list, optional\n",
    "        Labels for each history file. If None, filenames are used.\n",
    "    metric_name : str, optional\n",
    "        Metric to plot along with loss (default: 'f1_macro')\n",
    "    output_dir : str, optional\n",
    "        Directory to save plots (default: 'plots')\n",
    "    figsize : tuple, optional\n",
    "        Figure size (width, height) (default: (18, 14))\n",
    "    save_plot : bool, optional\n",
    "        Whether to save the plot to a file (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The created figure\n",
    "    \"\"\"\n",
    "    # Load history files\n",
    "    histories = []\n",
    "    for file in history_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Add epoch column if not present\n",
    "            if 'epoch' not in df.columns:\n",
    "                df['epoch'] = np.arange(1, len(df) + 1)\n",
    "            histories.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "            histories.append(None)\n",
    "    \n",
    "    # Get labels\n",
    "    if labels is None:\n",
    "        labels = [Path(file).stem for file in history_files]\n",
    "    \n",
    "    # Ensure we have the same number of labels as histories\n",
    "    if len(labels) < len(history_files):\n",
    "        # Add default labels if needed\n",
    "        additional_labels = [f\"Model {i+1}\" for i in range(len(labels), len(history_files))]\n",
    "        labels.extend(additional_labels)\n",
    "    \n",
    "    # Create figure and subplots - 2 rows, 2 columns\n",
    "    fig, axs = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Create references to the four subplots\n",
    "    ax_loss_first = axs[0, 0]    # Top-left for first model's loss\n",
    "    ax_metric_first = axs[1, 0]  # Bottom-left for first model's metric\n",
    "    ax_loss_others = axs[0, 1]   # Top-right for other models' loss\n",
    "    ax_metric_others = axs[1, 1] # Bottom-right for other models' metric\n",
    "    \n",
    "    # Set titles and labels\n",
    "    ax_loss_first.set_title(f'{labels[0]} - Training and Validation Loss', fontsize=16)\n",
    "    ax_loss_first.set_ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    ax_metric_first.set_title(f'{labels[0]} - Training and Validation {metric_name.upper()}', fontsize=16)\n",
    "    ax_metric_first.set_xlabel('Epoch', fontsize=14)\n",
    "    ax_metric_first.set_ylabel(metric_name.upper(), fontsize=14)\n",
    "    \n",
    "    ax_loss_others.set_title(f'Models Comparison - Training and Validation Loss', fontsize=16)\n",
    "    ax_loss_others.set_ylabel('Loss', fontsize=14)\n",
    "    \n",
    "    ax_metric_others.set_title(f'Models Comparison - Training and Validation {metric_name.upper()}', fontsize=16)\n",
    "    ax_metric_others.set_xlabel('Epoch', fontsize=14)\n",
    "    ax_metric_others.set_ylabel(metric_name.upper(), fontsize=14)\n",
    "    \n",
    "    # Colors for different histories\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    # First, collect all data to determine global min/max values for consistent y-axis limits\n",
    "    loss_min, loss_max = float('inf'), float('-inf')\n",
    "    metric_min, metric_max = float('inf'), float('-inf')\n",
    "    \n",
    "    for i, history_df in enumerate(histories):\n",
    "        if history_df is None:\n",
    "            continue\n",
    "            \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(history_df)\n",
    "        train_metric, val_metric = find_metric_columns(history_df, metric_name)\n",
    "        \n",
    "        # Update loss min/max\n",
    "        if train_loss:\n",
    "            loss_min = min(loss_min, history_df[train_loss].min())\n",
    "            loss_max = max(loss_max, history_df[train_loss].max())\n",
    "        \n",
    "        if val_loss:\n",
    "            loss_min = min(loss_min, history_df[val_loss].min())\n",
    "            loss_max = max(loss_max, history_df[val_loss].max())\n",
    "            \n",
    "        # Update metric min/max\n",
    "        if train_metric:\n",
    "            metric_min = min(metric_min, history_df[train_metric].min())\n",
    "            metric_max = max(metric_max, history_df[train_metric].max())\n",
    "        \n",
    "        if val_metric:\n",
    "            metric_min = min(metric_min, history_df[val_metric].min())\n",
    "            metric_max = max(metric_max, history_df[val_metric].max())\n",
    "    \n",
    "    # Add some padding to the ranges (5% padding)\n",
    "    loss_range = loss_max - loss_min\n",
    "    loss_min -= loss_range * 0.05\n",
    "    loss_max += loss_range * 0.05\n",
    "    \n",
    "    metric_range = metric_max - metric_min\n",
    "    metric_min -= metric_range * 0.05\n",
    "    metric_max += metric_range * 0.05\n",
    "    \n",
    "    # Plot first history\n",
    "    if histories[0] is not None:\n",
    "        first_df = histories[0]\n",
    "        color = colors[0]\n",
    "        \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(first_df)\n",
    "        train_metric, val_metric = find_metric_columns(first_df, metric_name)\n",
    "        \n",
    "        # Plot training and validation loss for first model\n",
    "        if train_loss:\n",
    "            ax_loss_first.plot(first_df['epoch'], first_df[train_loss], \n",
    "                            color=color, linestyle='-', marker='o', markersize=4, \n",
    "                            label='Train')\n",
    "        \n",
    "        if val_loss:\n",
    "            ax_loss_first.plot(first_df['epoch'], first_df[val_loss], \n",
    "                            color=color, linestyle='--', marker='s', markersize=4, \n",
    "                            label='Validation')\n",
    "        \n",
    "        # Plot training and validation metric for first model\n",
    "        if train_metric:\n",
    "            ax_metric_first.plot(first_df['epoch'], first_df[train_metric], \n",
    "                                color=color, linestyle='-', marker='o', markersize=4, \n",
    "                                label='Train')\n",
    "        \n",
    "        if val_metric:\n",
    "            ax_metric_first.plot(first_df['epoch'], first_df[val_metric], \n",
    "                                color=color, linestyle='--', marker='s', markersize=4, \n",
    "                                label='Validation')\n",
    "    \n",
    "    # Plot second and third histories (and any additional ones) together\n",
    "    for i in range(1, len(histories)):\n",
    "        history_df = histories[i]\n",
    "        label = labels[i]\n",
    "        \n",
    "        if history_df is None:\n",
    "            continue\n",
    "        \n",
    "        # Get color for this history\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Identify columns\n",
    "        train_loss, val_loss = identify_columns(history_df)\n",
    "        train_metric, val_metric = find_metric_columns(history_df, metric_name)\n",
    "        \n",
    "        # Plot training and validation loss\n",
    "        if train_loss:\n",
    "            ax_loss_others.plot(history_df['epoch'], history_df[train_loss], \n",
    "                              color=color, linestyle='-', marker='o', markersize=4, \n",
    "                              label=f'{label} - Train')\n",
    "        \n",
    "        if val_loss:\n",
    "            ax_loss_others.plot(history_df['epoch'], history_df[val_loss], \n",
    "                              color=color, linestyle='--', marker='s', markersize=4, \n",
    "                              label=f'{label} - Val')\n",
    "        \n",
    "        # Plot training and validation metric\n",
    "        if train_metric:\n",
    "            ax_metric_others.plot(history_df['epoch'], history_df[train_metric], \n",
    "                                color=color, linestyle='-', marker='o', markersize=4, \n",
    "                                label=f'{label} - Train')\n",
    "        \n",
    "        if val_metric:\n",
    "            ax_metric_others.plot(history_df['epoch'], history_df[val_metric], \n",
    "                                color=color, linestyle='--', marker='s', markersize=4, \n",
    "                                label=f'{label} - Val')\n",
    "    \n",
    "    # Set consistent y-limits for all loss plots and all metric plots\n",
    "    ax_loss_first.set_ylim(loss_min, loss_max)\n",
    "    ax_loss_others.set_ylim(loss_min, loss_max)\n",
    "    \n",
    "    ax_metric_first.set_ylim(metric_min, metric_max)\n",
    "    ax_metric_others.set_ylim(metric_min, metric_max)\n",
    "    \n",
    "    # Add legends\n",
    "    ax_loss_first.legend(loc='best', fontsize=12)\n",
    "    ax_metric_first.legend(loc='best', fontsize=12)\n",
    "    ax_loss_others.legend(loc='best', fontsize=12)\n",
    "    ax_metric_others.legend(loc='best', fontsize=12)\n",
    "    \n",
    "    # Set grid\n",
    "    ax_loss_first.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_metric_first.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_loss_others.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax_metric_others.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if requested\n",
    "    if save_plot:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(output_dir, f'training_history_split_{metric_name}.png')\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {output_path}\")\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_files = [config.MODEL_DIR / \"finetune_orig_history.csv\",\n",
    "                 config.MODEL_DIR / \"finetune_cross_round_1_history.csv\",\n",
    "                 config.MODEL_DIR / \"finetune_cross_round_2_history.csv\"\n",
    "]\n",
    "labels = [\"Fine-tuning on Natural Images\", \"Cross Fine-tuned (Round 1)\", \"Cross Fine-tuned (Round 2)\"]\n",
    "fig = plot_training_histories(history_files, labels, metric_name='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = create_regular_generator(TEST_DIR, with_augment=False, shuffle=False)\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "y_true = test_generator.classes\n",
    "\n",
    "model_paths = [MODEL_DIR / \"finetune_orig_best.h5\",\n",
    "               MODEL_DIR / \"finetune_cross_round_2.h5\"]\n",
    "y_prob = []\n",
    "y_pred = []\n",
    "for path in model_paths:\n",
    "    model = load_model_from_path(path)\n",
    "    y_prob_ = model.predict(test_generator)\n",
    "    y_prob.append(y_prob_)\n",
    "    y_pred.append(np.argmax(y_prob_, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "for ind in range(2):\n",
    "    cm = confusion_matrix(y_true, y_pred[ind])\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize by row (i.e., by the actual class)\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', cbar=False,\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names)\n",
    "    plt.title(f'Test Set Confusion Matrix (Test {ind+1})')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOT_DIR, f\"confusion_matrix_test_{ind+1}.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-class F1 + macro for each model\n",
    "all_f1_scores = []\n",
    "labels = [f\"Test {i+1}\" for i in range(len(y_pred))]\n",
    "colors = ['skyblue', 'salmon']                   # Bar colors per model\n",
    "highlight_colors = ['royalblue', 'orangered']    # Macro bar colors per model\n",
    "\n",
    "for y_pred_stage in y_pred:\n",
    "    class_report = classification_report(y_true, y_pred_stage, target_names=class_names, output_dict=True)\n",
    "    f1_scores = [class_report[class_name]['f1-score'] for class_name in class_names]\n",
    "    f1_macro = np.mean(f1_scores)\n",
    "    f1_scores.append(f1_macro)  # Add macro as 8th bar\n",
    "    all_f1_scores.append(f1_scores)\n",
    "\n",
    "# Convert to array for easier plotting\n",
    "all_f1_scores = np.array(all_f1_scores)  # shape: (num_models, 8)\n",
    "num_models, num_bars = all_f1_scores.shape\n",
    "bar_width = 0.35\n",
    "x = np.arange(num_bars)\n",
    "extended_class_names = class_names + ['F1 Macro']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(num_models):\n",
    "    # Set per-bar color list: all regular bars in model color, macro bar in highlight color\n",
    "    bar_colors = [colors[i]] * (num_bars - 1) + [highlight_colors[i]]\n",
    "    plt.bar(x + i * bar_width, all_f1_scores[i], width=bar_width, label=labels[i], color=bar_colors)\n",
    "\n",
    "# Format\n",
    "plt.xticks(x + bar_width * (num_models - 1) / 2, extended_class_names, rotation=45)\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"F1 Scores on Test Set\")\n",
    "plt.legend()\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"f1_scores_with_macro_highlighted.png\"), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "stage_labels = [\"Test 1\", \"Test 2\"]\n",
    "metric_labels = [\"Top-1 Accuracy\", \"Top-3 Accuracy\"]\n",
    "colors = ['skyblue', 'salmon']  # Stage 1 = blue, Stage 2 = red\n",
    "\n",
    "# Convert y_true once\n",
    "y_true_categorical = tf.keras.utils.to_categorical(y_true, num_classes=len(class_names))\n",
    "\n",
    "# Collect accuracies\n",
    "top1_list = []\n",
    "top3_list = []\n",
    "\n",
    "for y_prob_i in y_prob:\n",
    "    y_prob_tensor = tf.convert_to_tensor(y_prob_i)\n",
    "    top1 = categorical_accuracy(y_true_categorical, y_prob_tensor).numpy().mean()\n",
    "    top3 = top_k_categorical_accuracy(y_true_categorical, y_prob_tensor, k=3).numpy().mean()\n",
    "    top1_list.append(top1)\n",
    "    top3_list.append(top3)\n",
    "\n",
    "# Prepare data: rows = metrics, columns = stages\n",
    "accuracy_data = np.array([top1_list, top3_list])  # shape (2, num_stages)\n",
    "x = np.arange(len(metric_labels))\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Plot each stage\n",
    "for i in range(len(stage_labels)):\n",
    "    plt.bar(x + i * bar_width, accuracy_data[:, i],\n",
    "            width=bar_width, color=colors[i], label=stage_labels[i])\n",
    "    # Add annotations\n",
    "    for j in range(len(metric_labels)):\n",
    "        value = accuracy_data[j, i]\n",
    "        plt.text(x[j] + i * bar_width, value + 0.02, f\"{value:.2f}\", ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Format\n",
    "plt.xticks(x + bar_width / 2, metric_labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Top-1 and Top-3 Accuracy\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(PLOT_DIR, \"top_1_3_accuracy_grouped_by_metric.png\"), dpi=200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
